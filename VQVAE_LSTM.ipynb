{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import fluidsynth\n",
    "import pretty_midi\n",
    "\n",
    "import numpy as np\n",
    "import IPython.display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.callbacks import StochasticWeightAveraging\n",
    "\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from utils.data import *\n",
    "from utils.loss import *\n",
    "from utils.model import *\n",
    "from utils.metric import *\n",
    "from utils.common_utils import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init seed\n",
    "random_seed = 0\n",
    "pl.seed_everything(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize model with GPU\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device('cuda' if use_cuda else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './data/code_list_num_dict_512_kick.pkl'\n",
    "with open(path, 'rb') as f:\n",
    "    data = np.stack(pickle.load(f))\n",
    "                      \n",
    "print('data shape :', data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle and split\n",
    "num_data = len(data)\n",
    "num_train = int(num_data * 0.95)\n",
    "\n",
    "train_data = data[:num_train]\n",
    "val_data = data[num_train:]\n",
    "\n",
    "print('The number of train: %d' % len(train_data))\n",
    "print('The number of validation: %d' % len(val_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloader\n",
    "batch_size = 2048\n",
    "train_params = {'batch_size': batch_size,\n",
    "                'shuffle': True,\n",
    "                'pin_memory': True,\n",
    "                'num_workers': 4}\n",
    "\n",
    "val_params = train_params.copy()\n",
    "val_params['shuffle'] = False\n",
    "\n",
    "train_set = DataLoader(DatasetSamplerInt(train_data), **train_params)\n",
    "val_set = DataLoader(DatasetSamplerInt(val_data), **val_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "embed_size = 128\n",
    "hidden_size = 512\n",
    "vocab_size = 512\n",
    "\n",
    "model = LSTM(embed_size, hidden_size, vocab_size, num_layers=4)\n",
    "swa_callback = StochasticWeightAveraging(swa_epoch_start=0.7, swa_lrs=5e-5, annealing_epochs=10)\n",
    "checkpoint_callback = ModelCheckpoint(monitor='val_loss',\n",
    "                                      mode='max',\n",
    "                                      filename='LSTM-{epoch:02d}-{val_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# training\n",
    "trainer = pl.Trainer(gpus=[1],\n",
    "                     num_nodes=1,\n",
    "                     max_epochs=100,\n",
    "                     deterministic=True,\n",
    "                     default_root_dir='./model',\n",
    "                     callbacks=[swa_callback, checkpoint_callback])\n",
    "\n",
    "trainer.fit(model, train_set, val_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('best model path :', checkpoint_callback.best_model_path)\n",
    "print('final results :', trainer.logged_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get x1 prob\n",
    "num_classes = 512\n",
    "prob_x1 = np.sum(np.eye(num_classes)[data[:, 0]], axis=0)\n",
    "prob_x1 = prob_x1 / np.sum(prob_x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot parameters\n",
    "CHAR_FONT_SIZE = 15\n",
    "NUM_FONT_SIZE = 12\n",
    "WIDTH = 17\n",
    "HEIGHT = 5\n",
    "LABEL_PAD = 13\n",
    "\n",
    "# plotting\n",
    "plt.figure(figsize=(WIDTH, HEIGHT))\n",
    "plt.bar(np.arange(num_classes), prob_x1)\n",
    "plt.xticks(fontsize=NUM_FONT_SIZE)\n",
    "plt.yticks(fontsize=NUM_FONT_SIZE)\n",
    "plt.xlabel('Classes', fontsize=CHAR_FONT_SIZE, labelpad=LABEL_PAD)\n",
    "plt.ylabel('Count', fontsize=CHAR_FONT_SIZE, labelpad=LABEL_PAD)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load LSTM\n",
    "AR_model = LSTM(embed_size, hidden_size, vocab_size, num_layers=4)\n",
    "\n",
    "ckpt_path = './model/kick/num_dict_512/LSTM-epoch=98-val_loss=0.7666.ckpt'\n",
    "AR_model = AR_model.load_from_checkpoint(ckpt_path,\n",
    "                                         embed_size=embed_size,\n",
    "                                         hidden_size=hidden_size,\n",
    "                                         vocab_size=vocab_size,\n",
    "                                         num_layers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load VQ-VAE\n",
    "ch = 128\n",
    "num_pitch = 57\n",
    "latent_dim = 16\n",
    "num_embed = 512\n",
    "\n",
    "ckpt_path = './model/kick/num_dict_512/VQVAE-epoch=369-val_loss=0.0066.ckpt'\n",
    "VQVAE_model = VQVAE(ch, num_pitch, latent_dim, num_embed)\n",
    "VQVAE_model = VQVAE_model.load_from_checkpoint(ckpt_path,\n",
    "                                               ch=ch,\n",
    "                                               num_pitch=num_pitch,\n",
    "                                               num_embed=num_embed,\n",
    "                                               latent_dim=latent_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rejection Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load detector\n",
    "num_features = 28\n",
    "AE = AutoEncoder(num_features)\n",
    "\n",
    "center = [3.4064, -2.3389, -2.8335, -1.2972, -2.0128, -1.1937, 1.1904]\n",
    "center = torch.as_tensor(center)\n",
    "\n",
    "ckpt_path = './model/SVDD-epoch=562-val_loss=0.06.ckpt'\n",
    "SVDD_model = SVDD.load_from_checkpoint(ckpt_path, encoder=AE.encoder, center=center.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data generation under loop score\n",
    "gen_data =[]\n",
    "start_time = time()\n",
    "\n",
    "while True:\n",
    "    temp_code = sampling_code(batch_size, num_data//10, AR_model, \n",
    "                              num_classes, prob_x1, device, \n",
    "                              len_code=32, top_k=30, top_p=0, temp=0.7)\n",
    "\n",
    "    temp_data = sampling_from_code(batch_size, temp_code, VQVAE_model, device)\n",
    "    \n",
    "    for data in temp_data:\n",
    "        if loop_score(data[np.newaxis], SVDD_model, center) < 0.001:\n",
    "            gen_data.append(data)\n",
    "    \n",
    "    if len(gen_data) >= num_data:\n",
    "        break\n",
    "\n",
    "gen_data = np.stack(gen_data[:num_data])\n",
    "print('gen_data shape : %s (%0.3f sec)' % (gen_data.shape, time()-start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './data/midi_detected_strict_kick_pianoroll.pkl'\n",
    "with open(path, 'rb') as f:\n",
    "    original_data = np.stack(pickle.load(f))\n",
    "\n",
    "print('The number of data : %d' % len(original_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle and split\n",
    "num_data_origin = len(original_data)\n",
    "num_train_origin = int(num_data_origin * 0.8)\n",
    "\n",
    "original_data = original_data[:num_train_origin]\n",
    "\n",
    "print('The number of data: %d' % len(original_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# play demo\n",
    "pm = play_pianoroll(gen_data[1], fs=9)\n",
    "IPython.display.display(IPython.display.Audio(pm.fluidsynth(fs=16000), rate=16000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('loop_score : %f' % loop_score(gen_data, SVDD_model, center))\n",
    "print('unique_pitch : %f' % unique_pitch(gen_data))\n",
    "print('note_density : %f' % note_density(gen_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# precision & recall & diversity & coverage\n",
    "metrics = compute_prd(original_data, gen_data, k=5, repeat=10)\n",
    "\n",
    "print('precision : %0.3f (%0.3f)' % (np.mean(metrics['precision']), np.std(metrics['precision'])))\n",
    "print('recall : %0.3f (%0.3f)' % (np.mean(metrics['recall']), np.std(metrics['recall'])))\n",
    "print('density : %0.3f (%0.3f)' % (np.mean(metrics['density']), np.std(metrics['density'])))\n",
    "print('coverage : %0.3f (%0.3f)' % (np.mean(metrics['coverage']), np.std(metrics['coverage'])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
